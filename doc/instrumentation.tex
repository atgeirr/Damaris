
%==============================================================================%
%==============================================================================%
\section{Initializing and finalizing \Damaris{}}

This chapter will teach you how to instrument a simulation in order to 
use \Damaris. \Damaris{} requires the simulation to be based on MPI.

\subsection{Minimal configuration file}\label{sec:MinimalConfigFile}

Before actually instrumenting your code, an XML configuration file has 
to be created. This configuration file will contain some information about 
the simulation, the system, the data and the plugins that you want to use 
with \Damaris. Listing~\ref{InitAndFinitXML} presents a minimal configuration file.

\noindent\begin{minipage}{\textwidth}
\vspace{0.5cm}
\lstset{language=XML,caption={\Damaris{} initial architecture description},label=InitAndFinitXML}
\lstinputlisting[language=XML]{listings/init.xml}
\end{minipage}

The \texttt{simulation}'s \texttt{name} is used in different backends, for example to name trace files or
to be added on figures when using in situ visualization. The language (\texttt{"c"}, \texttt{"cpp"}, 
or \texttt{"fortran"}) is optional (default is \texttt{"c"}), it indicates in which language the simulation 
is written, so that fortran array layouts can be converted into C layouts.
The \texttt{domain} section provides the number of domains that a single process will handle. Some simulations
indeed split their variables into blocks (or domains) and distribute several blocks per process. If the number of
blocks is not the same on every process, it should correspond to the maximum number of blocks that a single process
may have to handle.
The \texttt{dedicated} section provides the number of dedicated cores in a node, and the number of dedicated nodes.
These different configurations are explained in more details in Chapter~\ref{modes}.

The \texttt{buffer} section provides the \texttt{name} and the \texttt{size} (in bytes) of 
the buffer that will be used for the simulation processes to communicate with the dedicated cores
or dedicated nodes. It can take an additional \texttt{type} attribute which can be either ``posix'' 
(by default) or ``sysv'', representing the type of share memory to be used.
POSIX shared memory will use the \function{shm\_open} function, while SYS-V shared memory will
use \function{shmget}. Some HPC operating systems indeed don't have one or the other type available.
Finally, an additional \texttt{blocks} attribute can be added to the buffer, 
taking an integer (e.g. \texttt{blocks="3"}) to indicate that several blocks of shared memory 
must be opened of the given \texttt{size}. This feature is useful in order to open a shared memory buffer 
with a size bigger than the maximum allowed by the operating system 
(most operating systems limit this size to 1 or 2~GB).
However, a variable can never have a size bigger than the block size 
(since blocks may not be contiguous).

The \texttt{queue} section provides the \texttt{name} and \texttt{size} 
(in number of messages) of the message queues
used to communicate between clients and dedicated resources. This is the remnant of
older versions of \Damaris{} in which the queue was handled in shared memory as well.
Simply keeping the default name and size works just fine.

Finally the \texttt{data} and \texttt{actions} sections will be described later.

\subsection{Instrumenting a simulation}

Now that a minimal configuration file has been provided, let's instrument our code.

\subsubsection{Writing the code}

Listings~\ref{InitAndFinitC} and~\ref{InitAndFinitF90} present the minimal code instrumentation
for a C and a Fortran code respectively. All \Damaris{} function calls must be placed after a first call to the
\function{damaris\_initialize} function (resp. \function{damaris\_initialize\_f} in Fortran). This function
takes as parameter an XML configuration file 
and a MPI communicator (generally MPI\_COMM\_WORLD).
The call to \function{damaris\_initialize} should be placed after the MPI initialization, as it uses
MPI functions. \function{damaris\_initialize} involves collective MPI communications: process 0 in
the provided communicator will load the XML file and broadcast its content to other processes
in the communicator, thus all the processes in the involved communicator should call this function.

Symmetrically a call
to \function{damaris\_finalize} (resp. \function{damaris\_finalize\_f}) frees the resources used by
\Damaris{} before the simulation finishes. It should be called before finalizing MPI.

These functions only prepare \Damaris{} but do not start the dedicated cores or nodes.
To start the dedicated resources, a call to \function{damaris\_start} (resp. \function{damaris\_start\_f})
is necessary. This function takes a pointer to an integer as parameter. \Damaris{} will automatically
analyze the platform on which the simulation runs and select cores to be either clients or servers.
On client processes, this function returns and the integer is set to a positive value. On server processes,
this function blocks and runs the server loop. It will only return when the clients have all called 
\function{damaris\_stop} (resp. \function{damaris\_stop\_f}), and the integer will be set to 0.
This integer can thus be used to select whether or not to run the simulation loop on a particular process.

Inside the simulation's main loop, a call to 
\function{damaris\_client\_comm\_get} gives a
communicator that the clients can use to communicate with one another (i.e., a replacement for
the MPI\_COMM\_WORLD that now includes dedicated resources). \textbf{It is crucial that the
simulation only uses this communicator and not MPI\_COMM\_WORLD, as global communicator}. 

\function{damaris\_end\_iteration}
(resp. \function{damaris\_end\_iteration\_f}) informs the dedicated cores that the current iteration 
has finished. \Damaris{} keeps track of the iteration number internally: 
this number is equal to 0 before the first call to \function{damaris\_end\_iteration}.
A call to \function{damaris\_end\_iteration} will update potentially connected backends such as VisIt.
As it involves collective communications, all the clients have to call this function.
These main functions are summarized in Table~\ref{tab:initFunctions}.

\begin{table}
\centering
\begin{tabular}{|l|}
	\hline
   \textbf{C functions} \\
   \hline
   \hline
   \function{int damaris\_initialize(const char* configfile, MPI\_Comm comm)}  \\
   \function{int damaris\_start(int* is\_client)} \\
   \function{int damaris\_client\_comm\_get(MPI\_Comm* comm)} \\
   \function{int damaris\_end\_iteration()}  \\
   \function{int damaris\_stop()} \\
   \function{int damaris\_finalize()} \\
   \hline
   \hline
   \textbf{Fortran functions} \\
   \hline
   \hline
   \function{damaris\_initialize\_f(character* configfile, MPI\_Fint comm, integer ierr)} \\
   \function{damaris\_start\_f(integer is\_client, integer ierr)} \\
   \function{damaris\_client\_comm\_get\_f(MPI\_Fint comm, integer ierr)} \\
   \function{damaris\_end\_iteration\_f(integer ierr)} \\
   \function{damaris\_stop\_f(integer ierr)} \\
   \function{damaris\_finalize\_f(integer ierr)} \\
   \hline
\end{tabular}\caption{Main \Damaris{} functions}\label{tab:initFunctions}
\end{table}

\noindent\begin{minipage}{\textwidth}
\lstset{language=C,caption={Initializing and finalizing \Damaris{}},label=InitAndFinitC}
\lstinputlisting[language=C]{listings/init.c}
\end{minipage}

\noindent\begin{minipage}{\textwidth}
\lstset{language=fortran,caption={Initializing and finalizing \Damaris{}},label=InitAndFinitF90}
\lstinputlisting[language=fortran]{listings/init.f90}
\end{minipage}

\textbf{Note:} \Damaris{} is not thread-safe yet. If your application uses an hybrid model
where different threads run on the same node, only one thread should call \function{damaris\_initialize} 
(and other \Damaris{} functions), and all the threads on a node should act as one single client.

\subsubsection{Compiling}

In order to compile your instrumented simulation, the \Damaris{} header files must be
provided, as well as the dependencies' headers. The following options must be passed to the compiler:

\begin{verbatim}
-I/damaris/install/prefix/include -I/path/to/dependencies/include
\end{verbatim}
If all dependencies and \Damaris{} have been installed in \installdir, for example, it becomes
\begin{verbatim}
-I$HOME/local/include
\end{verbatim}

Fortran programs may look for \texttt{damaris.mod}, which is also in the include directory
where \Damaris{} has been installed.

The linker will need the \Damaris{} library file \file{libdamaris.a} as well as other dependent libraries.
The following provides the necessary options for the linker:
\begin{verbatim}
-rdynamic
-L/damaris/install/prefix/lib -ldamaris -L/path/to/dependencies/lib \
-Wl,--whole-archive,-ldamaris,--no-whole-archive \
-lxerces-c \
-lboost_filesystem -lboost_system -lboost_date_time \
-lstdc++ -lrt -ldl
\end{verbatim}

The \texttt{-rdynamic} option asks the linker to add all symbols, not only used ones, 
to the dynamic symbol table. 
This option is needed for some uses of \texttt{dlopen} in \Damaris. It is possible that 
the option differs with some linkers.

\subsubsection{Running}

The deployment of a \Damaris-enabled simulation is that of a normal simulation:

\begin{verbatim}
	mpirun -np X -machinefile FILE ./sim
\end{verbatim}

Where X is the number of processes (in total), and FILE lists the hostnames on which to start the program.

\textbf{Note:} \Damaris{} doesn't use any a priori knowledge about rank-to-cores binding and is able
to start the proper number of servers at the right location even if process ranks are not consecutive 
in the nodes. It leverages the processor's name to group processes, or platform specific information 
such as the host's personality on BG/P. Yet, it is important that the same number of cores are used
in each node.

%==============================================================================%
%==============================================================================%
\section{Describing and writing data}

Now that we have a working simulation instrumented with \Damaris, it is time to write data.
This section explains how to describe data in the XML file and how to send it to \Damaris.
All the data items must be described in the XML configuration within the \texttt{<data>} section.

\subsection{Parameters}

Parameters are simple values associated with a name and a type. It allows to
easily change some important values of the simulation without changing all the variables
and objects that depend on them. Parameters can also be used as input of the simulation itself. 
Listing~\ref{parameterXML} shows how to define a parameter with a default value.
Parameters must be defined at the root of the \texttt{<data>}  section, not within nested groups
(later explained).

\noindent\begin{minipage}{\textwidth}
\vspace{0.5cm}
\lstset{language=XML,caption={Parameters descriptions},label=parameterXML}
\lstinputlisting[language=XML]{listings/parameter.xml}
\end{minipage}

Parameters must have a valid \texttt{name}, i.e., it should not contain any space or special symbols,
and should not start with a digit. Basically any name valid as variable name in C or Fortran is a valid
parameter name in \Damaris. The \texttt{type} attribute must be one of the types listed in 
Appendix~\ref{sec:types}. The provided \texttt{value} must be acceptable for the specified type. For instance
\texttt{value="xyz"} is not valid for an integer parameter.

Listing~\ref{parameterC} and~\ref{parameterF} show how to get and set parameters at run time
from the simulation. \textbf{Note:} when a process modifies a parameter, this modification is local and
not visible to other processes or to dedicated cores/nodes. The user is responsible for synchronizing all
processes when a parameter has to be changed globally.

\noindent\begin{minipage}{\textwidth}
\vspace{0.5cm}
\lstset{language=C,caption={Setting and getting parameters in C},label=parameterC}
\lstinputlisting[language=C]{listings/parameter.c}
\end{minipage}

\noindent\begin{minipage}{\textwidth}
\vspace{0.5cm}
\lstset{language=Fortran,caption={Setting and getting parameters in Fortran},label=parameterF}
\lstinputlisting[language=Fortran]{listings/parameter.f90}
\end{minipage}

Parameters are especially useful to define layouts, which are described in the next section.

\subsection{Layouts}

Layouts describe the shape of the data. \Damaris{} separates this description from the description
of the variables themselves since several variables can have the same layout.
A layout is characterized by a \texttt{name}, a base \texttt{type} and a list of \texttt{dimensions}.
Two examples are given in Listing~\ref{layoutXML}.

The \texttt{name} of a layout can be any string not including the ``/'' character 
(it can include special characters, white spaces, numbers, etc. even
though we advise to keep it simple and use classic C identifiers). 
The name cannot be the name of a basic type (such as \texttt{int}).
The \texttt{type} should be one of the basic types listed in Appendix~\ref{sec:types}.
Finally the list of dimensions is a comma-separated list of arithmetical expression featuring
+, -, *, /, \%, parenthesis, numbers and any defined int parameters (as of today, only int and integer 
parameters are allowed, short and long will produce an error).
In the following examples, the first layout has three dimensions, the second one has two dimensions.

When a layout depends on parameters, any modification of the parameters will automatically modify
the layout. Like parameters, the layout is only locally affected and the modification is not propagated to
other processes. Making a layout depend on a parameter and changing the parameter at run time 
can be very useful for particle-based simulations, where the number of particles is different at each process,
or simply to make the XML file independent of the simulation's size and avoid changing the parameters
every time the size changes.

The description of a layout will be used when writing a variable. Note that a modification in 
a layout at run time (through a modification of a parameter) will not affect previously written iterations of a
variable. It only affects the upcoming ones.

\noindent\begin{minipage}{\textwidth}
\vspace{0.5cm}
\lstset{language=XML,caption={Layouts descriptions},label=layoutXML}
\lstinputlisting[language=XML]{listings/layout.xml}
\end{minipage}

\textbf{Note:} \texttt{string} and \texttt{label} data types are variable-length types, thus
the dimension of the layout corresponds to the number of characters that the variable can store.

\subsection{Variables and groups}

\subsubsection{XML description}

Actual data is described through the \texttt{<variable>} and \texttt{<group>} nodes, which
allow to build a hierarchy of variables. An example is given in Listing~\ref{varXML}.
Each variable must be given a \texttt{name}, and be associated with a defined \texttt{layout}.
These are the two mandatory attributes. The \texttt{time-varying} attribute indicates whether
a variable is expected to be written at every iteration, or just once at the beginning of the
simulation (i.e., before the first call to \function{damaris\_end\_iteration}). 
The \texttt{visualizable} attribute indicates that a variable is visualizable
by visualization backends (for instance, coordinate arrays are not visualizable). 
In the following example, we expect the \texttt{x\_coordinates} variable
to be the coordinate of points in a rectilinear grid. This data is not itself visualizable, however
a rectilinear grid (later described in Chapter~\ref{chap:VisIt}) will be a visualizable object.
Note that one can use a basic type instead of a layout, as for the \texttt{simple var} variable: 
basic types are already interpreted as layouts.

\noindent\begin{minipage}{\textwidth}
\vspace{0.5cm}
\lstset{language=XML,caption={Variables and groups descriptions},label=varXML}
\lstinputlisting[language=XML]{listings/var.xml}
\end{minipage}

\subsubsection{Relative and absolute names}

Variables and layouts can be defined within groups. In Listing~\ref{varXML}, the relative name of the 
temperature variable is \emph{``temperature''}, while its absolute name is \emph{``my group/temperature''}.
The same goes for the name of layouts.

When \Damaris{} searches for a layout associated with a variable, it first looks inside
the group where the variable is defined, then in the parent group, and so on. It is thus possible to
refer to a layout either with its absolute name (if the layout is located in a different group) or
with a relative name if the layout can be found in the same group hierarchy.

\subsubsection{Writing full variables}

Now that the data is described, we can write it from the simulation. 
Listings~\ref{writingVarC} and~\ref{writingVarF90} present how to write a variable. 
The full name of the variable should be provided.

\noindent\begin{minipage}{\textwidth}
\vspace{0.5cm}
\lstset{language=C,caption={Writing a variable to \Damaris{} in C},label=writingVarC}
\lstinputlisting[language=C]{listings/write.c}
\end{minipage}

\noindent\begin{minipage}{\textwidth}
%\vspace{0.5cm}
\lstset{language=Fortran,caption={Writing a variable to \Damaris{} in Fortran},label=writingVarF90}
\lstinputlisting[language=Fortran]{listings/write.f90}
\end{minipage}

\subsubsection{Writing multiple domains}

By default, \Damaris{} expects one block of data per client and per variable.
It may be necessary for a client to write multiple blocks of a single variable. 
To do so, the number of domains have to be specified in the \texttt{domains} section of the 
XML file, as shown in Listing~\ref{writingBlocksXML}.

\noindent\begin{minipage}{\textwidth}
\vspace{0.5cm}
\lstset{language=XML,caption={Specifying multiple domains per client},label=writingBlocksXML}
\lstinputlisting[language=XML]{listings/blocks.xml}
\end{minipage}

This number is a maximum, a client may write less domains, but not more. Besides, if a client
writes $N$ domains, it must identify them from $0$ to $N-1$.
Listings~\ref{writingBlocksC} and~\ref{writingBlocksF90} show how to write multiple blocks.
Each block is expected to have the size and shape defined in the layout associated with the variable.

\noindent\begin{minipage}{\textwidth}
\vspace{0.5cm}
\lstset{language=C,caption={Writing a chunk of variable to \Damaris{} in C},label=writingBlocksC}
\lstinputlisting[language=C]{listings/blocks.c}
\end{minipage}

\noindent\begin{minipage}{\textwidth}
%\vspace{0.5cm}
\lstset{language=Fortran,caption={Writing a chunk of variable to \Damaris{} in Fortran},label=writingBlocksF90}
\lstinputlisting[language=Fortran]{listings/blocks.f90}
\end{minipage}

\subsubsection{Direct access to the shared memory}

The API presented above has the disadvantage of copying local data into the shared memory.
A more efficient way of proceeding consists of getting direct access to the shared memory.
Listings~\ref{allocateC} and~\ref{allocateF90} present this capability.

If \function{damaris\_alloc} fails, it will produce a NULL pointer. Otherwise, a valid pointer
to an allocated region of shared memory is produced.
After writing the data to the returned buffer, a call to \function{damaris\_commit} will notify the server
of the presence of new data. After this call, the user is not expected to write the data anymore.
Finally \function{damaris\_clear} indicates that the client delegates full responsibility to the
servers for the data. It is not supposed to be read nor written anymore.
Equivalent functions (\function{damaris\_alloc\_block} and \function{damaris\_alloc\_block\_f}) 
exist to allocate blocks when each client handles multiple domains.

\noindent\begin{minipage}{\textwidth}
\vspace{0.5cm}
\lstset{language=C,caption={Allocate a variable in shared memory from C},label=allocateC}
\lstinputlisting[language=C]{listings/alloc.c}
\end{minipage}

\noindent\begin{minipage}{\textwidth}
%\vspace{0.5cm}
\lstset{language=Fortran,caption={Allocate a variable in shared memory from Fortran},label=allocateF90}
\lstinputlisting[language=Fortran]{listings/alloc.f90}
\end{minipage}

Du to the C/Fortran interface, the use of \function{damaris\_alloc\_f} is more complex in Fortran than in C.
The \Damaris{} module should be used (this module is located where \Damaris{} has been installed). 
An additional call to \function{c\_f\_pointer} is mandatory to convert the returned C pointer
into a valid Fortran array. This function takes a shape array to provide the extents along each dimensions.

Note that it can be desirable,
in order to implement efficient double-buffering, that a client waits some iterations before actually committing 
a variable. Different versions of the function are available:
\begin{itemize}
\item \function{damaris\_commit(const char* var)} : \\ commits all the blocks of the current iteration;
\item \function{damaris\_commit\_block(const char* var, int32\_t block)} : \\ commits one specific block of the current iteration;
\item \function{damaris\_commit\_iteration(const char* var, int32\_t iteration)} : \\ commits all the blocks of a specific iteration;
\item \function{damaris\_commit\_block\_iteration(const char* var, int32\_t block, int32\_t iteration)} \\ commits one specific block of a specific iteration.
\end{itemize}

Equivalent functions are available in Fortran: \function{damaris\_commit\_f},
\function{damaris\_commit\_block\_f},\\ \function{damaris\_commit\_iteration\_f} and \function{damaris\_commit\_block\_iteration\_f}.
All take an \emph{ierr} integer in addition to the same parameters as the C functions.

\subsubsection{Error handling}\label{sec:error}

Since \function{damaris\_write} and \function{damaris\_alloc} (and corresponding functions for writing or
allocating blocks) both require to allocate a portion of shared memory, a call may fail if the shared memory
is full. When this happens \emph{all subsequent calls to \function{damaris\_write} or \function{damaris\_alloc}
up to the next call to \function{damaris\_end\_iteration} will return with an error without attempting
to allocate memory}. A special signal will be sent to all the servers when
calling \function{damaris\_end\_iteration}, which informs the servers that some data are missing
for this iteration. By default, the dedicated cores will not update potentially connected visualization backends,
and will delete from memory the data that has been written successfully for this iteration.
Overwriting this default behavior will be covered in the next chapter.

